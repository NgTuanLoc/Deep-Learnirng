{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo máy ảo bằng Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kill kernel cũ đi đồng thời tạo kết nối mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt các thư viện cần thiết và chứng thực"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "# !apt-get update -qq 2>&1 > /dev/null\n",
    "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# creds = GoogleCredentials.get_application_default()\n",
    "# import getpass\n",
    "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "# vcode = getpass.getpass()\n",
    "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt các thư viện liên quan đến việc kết nối Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p drive\n",
    "# !google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân tích cảm xúc với LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment này, chúng ta sẽ dùng mạng LSTM để giải quyết bài toán phân tích cảm xúc (Sentiment Analysis) trên tập dữ liệu văn bản. Nếu nhìn theo kiểu black box, đầu vào của bài toán là một câu hoặc đoạn văn bản và đầu ra là trạng thái tích cực, tiêu cực hay trung hoà (positive - negative - neutral). Trong phạm vi của assignment này, chúng ta chỉ quan tâm đến hai trạng thái cảm xúc là positive và negative.\n",
    "\n",
    "![caption](Images/input_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Góc nhìn Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như chúng ta giữ nguyên định dạng đầu vào là chuỗi ký tự thì rất khó để thực hiện các thao tác biến đổi như tích vô hướng (dot product) hoặc các thuật toán trên mạng neural network như backpropagation. Thay vì dữ liệu đầu vào là một chuỗi, chúng ta cần chuyển đổi các từ trong tập từ điển sang dạng vector số học trong đó có thể thực hiện được các phép toán nêu trên.\n",
    "\n",
    "![caption](Images/word2vec.png)\n",
    "\n",
    "Trong hình minh hoạ ở trên, ta có thể hình dung dữ liệu đầu vào của thuật toán phân tích cảm xúc là một ma trận 16 x D chiều. Trong đó 16 là số lượng từ trong câu và D là số chiều của không gian vector để biểu diễn từ. Để ánh xạ từ một từ sang một vector, chúng ta sử dụng ma trận word embedding như đã thực hiện trong bài Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tập dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment này, chúng tôi sử dụng tập dữ liệu review trên trang Foody với khoảng 30,000 mẫu được gán nhãn. Trong đó có 15,000 mẫu positive và 15,000 mẫu negative. Nguồn: https://streetcodevn.com/blog/dataset. Tập dữ liệu này đã được đính kèm trong thư mục của assignment 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các bước để huấn luyện trên mạng RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có 5 bước chính để giải quyết bài toán phân tích cảm xúc trong văn bản:\n",
    "\n",
    "    1) Huấn luyện một mô hình phát sinh ra vector từ (như mô hình Word2Vec) hoặc tải lên các vector từ tiền huấn luyện.\n",
    "    2) Tạo ma trận ID cho tập dữ liệu huấn luyện\n",
    "    3) Tạo mô hình RNN với các đơn vị LSTM, sử dụng tensorflow\n",
    "    4) Huấn luyện mô hình RNN với dữ liệu ma trận đã tạo ở bước 2\n",
    "    5) Đánh giá mô hình đã huấn luyện với tập test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load tập từ vựng và ma trận word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, để có thể biến đổi một từ thành một vector, chúng ta sử dụng mô hình đã được huấn luyện trước đó (pretrained model). Mô hình đã train trước đó cho tiếng Việt được lấy ở đây: https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.vi.300.vec.gz\n",
    "\n",
    "Tuy nhiên, số lượng từ vựng tiếng Việt được huấn luyện rất lớn, khoảng 2M từ. Mỗi từ được biểu diễn dưới dạng một vector 300 chiều. Với kích thước gốc của ma trận word embedding như vậy sẽ gây khó khăn cho việc load dữ liệu cũng như đưa vào thư viện tensorflow để huấn luyện nên chúng tôi đã tối giản lại với số lượng từ tối thiểu để có thể chạy được trên tập dữ liệu review về đồ ăn của Foody.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# LƯU Ý: CẦN PHẢI CHỈNH LẠI ĐƯỜNG DẪN NÀY THÀNH THƯ MỤC CHỨA CÁC FILE ASSIGNMENT3\n",
    "# CHỮ 'drive' có nghĩa là thư mục mặc định của Google drive\n",
    "currentDir = ''\n",
    "\n",
    "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "print('Simplified vocabulary loaded!')\n",
    "wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để chắc chắn mọi dữ liệu được load lên một cách chính xác, chúng ta cần kiểm tra xem số lượng từ trong từ điển rút gọn và số chiều của ma trận word embedding có khớp với nhau hay không? Trong trường hợp này số từ mà chúng tôi giữ lại là 19,899 và số chiều trong không gian biểu diễn là 300 chiều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary:  19899\n",
      "Size of the word embedding matrix:  (19899, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Size of the vocabulary: ', len(wordsList))\n",
    "print('Size of the word embedding matrix: ', wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec trên một từ đơn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để có thể xác định được vector biểu diễn của một từ tiếng Việt. Đầu tiên chúng ta sẽ xác định xem vị trí của từ đó trong wordsList. Sau đó lấy vector ở dòng tương ứng trên trên ma trận wordVectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of `ngon` in wordsList:  14598\n"
     ]
    }
   ],
   "source": [
    "ngon_idx = wordsList.index('ngon')\n",
    "print('Index of `ngon` in wordsList: ', ngon_idx)\n",
    "# ngon_vec = wordVectors[ngon_idx]\n",
    "# print('Vector representation of `ngon` is: ', ngon_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.1: Word2Vec để biểu diễn một đoạn văn bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nâng cấp hơn so với phiên bản Word2Vec cho từ đơn, phần này chúng ta sẽ biểu diễn một câu dưới dạng một ma trận gồm các vector biểu diễn của từng từ chồng lên nhau.\n",
    "\n",
    "Ví dụ như chúng ta muốn biểu diễn câu \"Món này ăn hoài không biết chán\". Đầu tiên, với mỗi từ trong câu ta sẽ tìm chỉ số tương ứng trong từ điển và lưu vào vector đặt tên là 'sentenceIndexes'. Sau đó, chúng ta có thể sử dụng hàm tra cứu ma trận word embedding của thư viện Tensorflow tf.nn.embedding_lookup để tra các vector tại các chỉ số trong 'sentenceIndexes'. Như vậy nếu chúng ta sử dụng tối đa 10 từ để lưu trữ cho một câu thì ma trận biểu diễn cho câu sẽ là một ma trận kích thước 10 x 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Row index for each word:  [  119  8136  4884 18791 16614 15951  3371     0     0     0]\n",
      "Sentence representation of word vectors:\n",
      "(10, 300)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "maxSeqLength = 10   #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "sentenceIndexes = np.zeros((maxSeqLength), dtype='int32')\n",
    "\n",
    "# TODO 3.1: Gán chỉ số của các từ trong câu và 'sentenceIndexes'\n",
    "my_sentence = 'Món này ăn hoài không biết chán'\n",
    "def return_words_id(sentence):\n",
    "    return [wordsList.index(id) for id in sentence.lower().split(\" \")]\n",
    "\n",
    "for i, item in enumerate(return_words_id(my_sentence)):\n",
    "    sentenceIndexes[i] = item\n",
    "\n",
    "# Các chỉ số 7, 8, 9 của sentenceIndexes  vẫn được gán bằng 0 như cũ\n",
    "print(sentenceIndexes.shape)\n",
    "print('Row index for each word: ', sentenceIndexes)\n",
    "\n",
    "# Ma trận biểu diễn:\n",
    "print('Sentence representation of word vectors:')\n",
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,sentenceIndexes).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như thực hiện đúng thì vector 'sentenceIndexes' sẽ có giá trị là: [119, 8136, 4884, 18791, 16614, 15951, 3371, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Khảo sát tập dữ liệu huấn luyện và tạo ma trận ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment 3, chúng tôi sử dụng tập dữ liệu lấy từ trang web Foody trên miền dữ liệu liên quan đến ẩm thực. Tập dữ liệu bao gôm 15.000 review tích cực đặt trong thư mục 'positiveReviews' và 15.000 review tiêu cực đặt trong thư mục 'negativeReviews'. Do khối lượng dữ liệu lớn, nếu chúng ta chọn số lượng từ tối đa (maxSeqLength) quá cao thì sẽ bị lãng phí khi biểu diễn ở những câu review quá ngắn. Ngược lại, nếu sử dụng số lượng từ tối đa quá ít thì sẽ bị bỏ lỡ những từ quan trọng giúp cho việc phân tích cảm xúc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 30000\n",
      "The total number of words in the files is 1770824\n",
      "The average number of words in the files is 59.02746666666667\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positiveReviews/27869.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positiveFiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11010"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positiveFiles.index('positiveReviews/1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể sử dụng thư viện Matplot để minh hoạ phân bố về chiều dài của các câu review trong tập dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa trên biểu đồ histogram ở trên chúng ta có thể thấy là 180 là kết quả tương đối hợp lý. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "# import re\n",
    "# strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
    "\n",
    "# def cleanSentences(string):\n",
    "#     string = string.lower().replace(\"<br />\", \" \")\n",
    "#     return re.sub(strip_special_chars, \"\", string.lower())\n",
    "import re\n",
    "\n",
    "def remove_punc_generator(string):\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for ele in string:  \n",
    "        if ele in punc:  \n",
    "            string = string.replace(ele, \" \") \n",
    "    yield string\n",
    " \n",
    " \n",
    "# sample = test\n",
    " \n",
    "# sample = remove_punc_generator(sample)\n",
    "# re.findall(r'\\S+', next(sample))\n",
    "def cleanSentences(string):\n",
    "    string = string.lower()\n",
    "    string = remove_punc_generator(string)\n",
    "#     string = \n",
    "    return re.findall(r'\\S+', next(string))\n",
    "# cleanSentences(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để có cảm nhận rõ hơn về dữ liệu, chúng ta có thể hiển thị một số review bất kỳ như sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A positive sentence: \n",
      "mình biết quán qua foody và mới đi ăn hôm qua luôn mình gọi kem xôi xoài 27k xúc xích và phô mai que không nhớ giá\n",
      "kem xôi hơi bé so với mình tưởng tượng cục xôi đừog kính khoảng 5cm còn kem thì bé hơn xíu nhưng vị ăn rất ngon ban đầu ăn thấy hơi ngọt nhưng nếu ăn chung với đậu phộng nữa thì tuyệt vời luôn 👍 👍\n",
      "xúc xích với phô mai que beo béo nchung là ngon 💕 💕\n",
      "phục vụ thì không có gì phải chê cả từ chị ở quầy đến mấy anh phục vụ trên lầu cũng rất vui vẻ niềm nở không gian quán sạch sẽ thoáng mát và decor cũng đẹp\n",
      "chỉ có 1 tí mình muốn góp ý là bàn có kiến bu khá là nhiều vậy thôii nếu có dịp thì chắc chắn mình sẽ tới tiếp 👍 👍\n",
      "A negative sentence: \n",
      "Quán đồ_ăn khá ngon . . nhưng phục_vụ không tốt . . chỉ lo sửa_soạn . . không để_ý tới khách . . đồ_ăn làm rất lâu trong khi chỉ có 2 bàn . . gần 1 tiếng mà chưa ra . . trong khi chỉ có rau xào và gỏi . khi mình đợi quá lâu nên tính tiền thì quản_lý có thái_độ khó_chịu . . không thích thái_độ phục_vụ 1 chút nào . . đây là lần cuối_cùng ghé quán . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('A positive sentence: ')\n",
    "fname = positiveFiles[1] # Randomly select a positive file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    id \n",
    "    test = []\n",
    "    for lines in f:\n",
    "        cleanedLine = cleanSentences(lines)\n",
    "        print(' '.join(cleanedLine))\n",
    "#         cleanedLine = cleanSentences(line)\n",
    "\n",
    "print('A negative sentence: ')\n",
    "fname = negativeFiles[0] # Randomly select a negative file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn hoá văn bản và tách từ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để tiết kiệm công sức và cũng nằm ngoài phạm vi của khoá học, chúng tôi đã chuẩn bị sẵn tập dữ liệu đã được tách từ. Giữa hai từ có thể ghép lại để tạo thành một khái niệm mới chúng tôi sử dụng ký tự '_' để nối các từ đó. Ví dụ: 'sinh_viên', 'sinh_học'.\n",
    "\n",
    "Chúng tôi chuẩn bị sẵn các hàm chuẩn hoá văn bản nhằm loại bỏ các ký tự đặc biệt. Tham khảo ở hàm 'cleanSentences'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta sẽ biểu diễn 30.000 review dưới dạng các chỉ số của các từ. Tập dữ liệu positive và negative sẽ được tính hợp lại thành một ma trận 30000x180. Trong đó 30000 là số lượng review và 180 là số lượng từ tối đa cho một câu. Do bước chuẩn bị này tốn khá nhiều tài nguyên tính toán nên sau khi tính toán xong, chúng ta sẽ lưu lại để sử dụng cho những lần chạy thí nghiệm sau. Ma trận lưu trữ các chỉ số này là: 'ids'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.2: xác định chỉ số của từng từ trong review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này chúng ta sẽ tiến hành tra cứu từng từ trong review, sau đó gán vào ma trận 'ids'. Trong đó chỉ số dòng của ma trận tương ứng với file review, chỉ số cột của ma trận tương ứng với một từ của review. Trường hợp từ nào không có trong tập từ điển thì ta sẽ gán bằng chỉ số của từ 'UNK' (unknow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files are indexed!\n",
      "Negative files are indexed!\n"
     ]
    }
   ],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "nFiles = 0\n",
    "# Index of Unknow word\n",
    "unk_idx = wordsList.index('UNK')\n",
    "\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
    "        nIndexes = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "#         split = cleanedLine.split()\n",
    "        split = cleanedLine\n",
    "        for word in split:\n",
    "            # TODO 3.2: Nếu 'word' thuộc tập 'wordsList' thì gán chỉ số của 'word' vào ma trận ids\n",
    "            if (word in wordsList):\n",
    "                ids[nFiles, nIndexes] = wordsList.index(word)\n",
    "            # Ngược lại: gán 'unk_idx' vào ma trận ids\n",
    "            else:\n",
    "                ids[nFiles, nIndexes] = unk_idx\n",
    "                \n",
    "            nIndexes = nIndexes + 1\n",
    "            if nIndexes >= maxSeqLength:\n",
    "                break\n",
    "        nFiles = nFiles + 1 \n",
    "print('Positive files are indexed!')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding=\"utf-8\") as f:\n",
    "        nIndexes = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "#         split = cleanedLine.split()\n",
    "        split = cleanedLine\n",
    "        for word in split:\n",
    "            if (word in wordsList):\n",
    "                ids[nFiles, nIndexes] = wordsList.index(word)\n",
    "            # Ngược lại: gán 'unk_idx' vào ma trận ids\n",
    "            else:\n",
    "                ids[nFiles, nIndexes] = unk_idx\n",
    "                        \n",
    "            nIndexes = nIndexes + 1\n",
    "            if nIndexes >= maxSeqLength:\n",
    "                break\n",
    "        nFiles = nFiles + 1 \n",
    "print('Negative files are indexed!')\n",
    "\n",
    "# Save ids Matrix for future uses.\n",
    "np.save(os.path.join(currentDir,'idsMatrix.npy'), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indexes of the first review:  [ 8589 18694  1906  4222  1232  5284 10661 11694 11994 18784 18569 15927\n",
      " 16416 13174  9821 14794  8884  6443  5767  8589 18423 12526 15570  5596\n",
      "   799 11060  4222 16893 13078  8136  3364  4222  1232  4756 15200  4884\n",
      "  8885  3553  9782  1232   616  7357 10606   579 15522  2219 15092 14855\n",
      " 15253  4884  3364  5519  4558  9649  4570 14653 15522 10959 12973 14855\n",
      "  4884 16614  2212  4884  7155 11577  4222  5767 15076 12225 10774  1218\n",
      "  2876 19584  4558 18839   255 13452  5013  4950  1825 10642 17292 11895\n",
      "   803 11060 16760  1906 15253 14598 15253  1047  5668  4884 10642 12225\n",
      "  7090 17292 18109 13078 16334  1238  3364  5519  3167 11318  3553 14967\n",
      "  4964 12067  2902  6544  6548  2997 14855  7446  8038 11440  1345  4950\n",
      "  1825  5767   803 11060 18791  5013     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# LƯU Ý: Bước thực hiện trên tương đối mất thời gian.\n",
    "# Trường hợp đã tính toán và lưu ma trận 'ids' rồi thì ta có thể load lên để sử dụng luôn\n",
    "ids = np.load(os.path.join(currentDir,'idsMatrix.npy'))\n",
    "print('Word indexes of the first review: ', ids[11010])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như quá trình chuyển từ câu dạng văn bảng sang vector các chỉ số trong từ điển ở trên đúng thì ids[0] sẽ nhận giá trị: [19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821 ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  [19898, 1906, 4454, 5284, 10661, 11694, 11994, 18784, 18569, 18619, 13174, 9821]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Món này dự là sẽ thành món hot mùa lễ_hội này đó nha . Miếng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK bán cơm_chiên nằm ngay đầu đường vào khu dân_cư metro chạy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([wordsList[i] for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xe đẩy bán cơm chiên nằm ngay đầu đường vào khu dân cư metro chạy từ ngoài vô là xe thứ hai nhé mình hay mua cơm của chị này lắm cơm chiên mềm nóng ăn chung với trứng chiên lạp xưởng thịt heo và chà bông nên vừa ăn lắm mà có thêm dưa leo và cà chua nên ăn không ngán ăn xong hợp cơm là bao no đến trưa giá chủ có 10 1 hộp à buổi sáng rất đông người ghé mua vì bán vừa ngon vừa rẻ lại ăn rất no tuy đông nhưng chị làm nhanh lắm mà nói chuyện với khách cũng vui vẻ lịch sự nữa nên lần nào đi ngang buổi sáng là ghé mua hoài à ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([wordsList[i] for i in ids[11010]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mình biết quán qua foody và mới đi ăn hôm qua luôn mình gọi kem xôi xoài 27k xúc xích và phô mai que không nhớ giá ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm ăm'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([wordsList[i] for i in ids[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xây dựng hàm lấy dữ liệu train và test theo từng batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây chúng tôi xây dựng các hàm để lấy dữ liệu train và test theo từng batch. Bạn hãy giải thích tại sao lại có các con số 13999, 14999, 15999, 29999 nhé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tập có nhãn tích cực có id từ 0 tới 13 999\n",
    "- Tập có nhãn tiêu cực có id từ 15 999 tới 29 999\n",
    "- Lấy batch size là 15 999 - 13 999 = 2000 mẫu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            # Pick positive samples randomly\n",
    "            num = randint(1,13999)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            # Pick negative samples randomly\n",
    "            num = randint(15999,29999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(13999,15999)\n",
    "        if (num <= 14999):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Xây dựng RNN Model với Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên chúng tôi sẽ khởi tạo các tham số cho mô hình mạng RNN với các cell là các LSTM. Kiến trúc mạng ở đây bao gồm 128 đơn vị cho mỗi lớp, số lượng layer là 2, số lượng phân lớp là 2 và số vòng lặp khi huấn luyện là 30000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paramters\n",
    "numDimensions = 300\n",
    "batchSize = 64\n",
    "lstmUnits = 128\n",
    "nLayers = 2\n",
    "numClasses = 2\n",
    "iterations = 30000\n",
    "maxSeqLength = 180 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để lưu trữ dữ liệu input và ouput, chúng ta sẽ sử dụng hai kiểu dữ liệu placeholder. Một trong những điều quan trọng nhất khi khởi tạo các biến input và output này là xác định kích thước của các tensor. Mỗi output của mạng (hay còn gọi là label) sẽ là một vector one hot với hai giá trị tương ứng với hai loại cảm xúc: [1, 0] cho positive và [0, 1] cho negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/data_batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.3: Xác định input và output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khởi tạo hai biến 'inputs' và 'labels' bằng kiểu placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# TODO 3.3: Khởi tạo hai biến 'inputs' và 'labels'\n",
    "inputs = tf.placeholder(tf.int32, shape=(batchSize, maxSeqLength))\n",
    "labels = tf.placeholder(tf.float32, shape=(batchSize, numClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(64, 180) dtype=int32>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(64, 2) dtype=float32>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó tạo dữ liệu word vector từ khối dữ liệu đầu vào với ma trận word embedding. Nếu như quá trình khởi tạo đúng thì sẽ tạo ra các kiểu dữ liệu sau:\n",
    "labels --> Tensor(\"Placeholder:0\", shape=(64, 2), dtype=float32)\n",
    "inputs --> Tensor(\"Placeholder_1:0\", shape=(64, 10), dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19899, 300)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(64, 180) dtype=int32>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.nn.embedding_lookup(wordVectors, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(64, 180, 300) dtype=float32>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy sau bước này chúng ta đã có dữ liệu để đưa vào mạng mạng các LSTM. Để khởi tạo một LSTM chúng ta sử dụng hàm tf.nn.rnn_cell.BasicLSTMCell. Hàm này cần tham số đầu vào là số lượng đơn vị muốn khởi tạo. Đây chính là một hyperparamter đã được khởi tạo trước đó.\n",
    "Để chống lại việc overfitting, chúng ta sử dụng lớp dropout. \n",
    "\n",
    "Để tăng tính phức tạp cho kiến trúc mạng chúng ta chồng các lớp LSTM lên nhau (Stack LSTM Layers). Trong trường hợp này chúng ta sử dụng 2 lớp LSTM. Việc chồng thêm các lớp LSTM sẽ giúp cho mô hình có khả năng nhớ nhiều thông tin hơn nhưng đồng thời cũng làm tăng số lượng tham số khi huấn luyện. Điều này cũng có nghĩa là sẽ làm tăng thời gian huấn luyện cũng như là cần thêm nhiều dữ liệu hơn.\n",
    "\n",
    "Cuối cùng là đưa toàn bộ dữ liệu đầu vào vào mạng các LSTM sử dụng hàm tf.nn.dynamic_rnn. Chi tiết kiến trúc mạng LSTM sử dụng cho bài tập này được mô tả trong hình sau:\n",
    "\n",
    "![caption](Images/architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:702: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "# Khởi tạo một LSTM layer với 'lstmUnits' unit sử dụng hàm tf.contrib.rnn.BasicLSTMCell\n",
    "# Sau đó tạo một lớp dropout để chống overfitting với hệ số out_keep_prob bằng 0.75\n",
    "# Sử dụng hàm tf.contrib.rnn.DropoutWrapper\n",
    "def make_cell():\n",
    "    lstmCell = tf.nn.rnn_cell.BasicLSTMCell(lstmUnits)\n",
    "    lstmCell = tf.nn.rnn_cell.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)    \n",
    "    return lstmCell\n",
    "\n",
    "\n",
    "def generate_a_lstm_layer(data, num_layers):\n",
    "    # stackedLstm = tf.nn.rnn_cell.MultiRNNCell([lstmCell, lstmCell])\n",
    "    stackedLstm = tf.nn.rnn_cell.MultiRNNCell([make_cell() for _ in range(num_layers)])\n",
    "\n",
    "    dyn_rnn_outputs, state = tf.nn.dynamic_rnn(stackedLstm, data, dtype=tf.float32)\n",
    "    return dyn_rnn_outputs\n",
    "\n",
    "# Sau khi đã có hàm tạo một LSTM Layer, ta sử dụng hàm này để chồng các LSTM lên\n",
    "# Stack các LSTM layer với hàm tf.nn.rnn_cell.MultiRNNCell\n",
    "...\n",
    "# Feed data variable vào mạng LSTM sử dụng hàm tf.nn.dynamic_rnn\n",
    "...\n",
    "# print(outputs)\n",
    "\n",
    "outputs = generate_a_lstm_layer(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(64, 180, 128) dtype=float32>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi ra khỏi mạng LSTM, biến outputs sẽ là một tensor có kích thước [batchSize x maxSeqLength x lstmUnits], cụ thể là [64 x 180 x 128]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó, chúng ta chỉ lấy dữ liệu ở LSTM cell cuối cùng và cho đi qua lớp kết nối đầy đủ để phân loại thành 2 trạng thái. Chỉ số của LSTM cell cuối cùng là 179 (do có 180 cell theo chiều ngang)  nên để có thể lấy được giá trị ta sẽ chuyển vị về tensor có kích thước [maxSeqLength x batchSize x lstmUnits] hay [180 x 64 x 128]. Sử dụng hàm tf.gather để lấy tensor thứ 179 có kích thước [64 x 128] bao gồm 64 mẫu vector 128 chiều. Vector 128 chiều này sẽ được đưa vào lớp fully connected để chuyển đổi về vector 2 chiều tương ứng với 2 trạng thái.\n",
    "\n",
    "Lớp kết nối đầy đủ bao gồm các bộ tham số 'weight' và 'bias' để thực hiện việc dự đoán kết quả. Bước này chính là tạo một lớp Fully Connected như trong sơ đồ kiến trúc mạng LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "\n",
    "# Lấy giá trị output tại LSTM cell cuối cùng\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "# last = tf.gather(outputs, int(value.get_shape()[0]) - 1)\n",
    "last = tf.gather(outputs, 180 - 1)\n",
    "# Đưa qua mạng Fully Connected mà không có activation function\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để xác định độ chính xác của hệ thống, ta đếm số lượng labels khớp với giá trị dự đoán (prediction). Sau đó tính độ chính xác bằng cách tính giá trị trung bình của các kết quả trả về đúng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctResult = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctResult, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó chúng ta sẽ xác định hàm độ lỗi sử dụng softmax cross entropy được tính từ dữ liệu dự đoán và tập labels. Cuối cùng là chọn thuật toán tối ưu với tham số learning rate mặc định là 0.001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sử dụng Tensorboard để visualize kết quả"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong quá trình huấn luyện, chương trình sẽ ghi log về độ lỗi và độ chính xác trên tập train vào thư mục 'tensorboard', lưu lại model sau mỗi 2000 vòng lặp ở thư mục 'models'. Việc huấn luyện trên 30,000 vòng lặp mất khoảng vài tiếng với GPU K80 được cung cấp bởi Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với mỗi vòng lặp, ta sẽ lấy ra một batch dữ liệu train để đưa vào mạng sử dụng `feed_dict`. với các tham số input và label là các placeholders. Bước huấn luyện này được lặp lại cho đến khi hết số lần cần huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/pretrained_lstm.ckpt-2000\n",
      "saved to models/pretrained_lstm.ckpt-4000\n",
      "saved to models/pretrained_lstm.ckpt-6000\n",
      "saved to models/pretrained_lstm.ckpt-8000\n",
      "saved to models/pretrained_lstm.ckpt-10000\n",
      "WARNING:tensorflow:From /Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py:968: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "saved to models/pretrained_lstm.ckpt-12000\n",
      "saved to models/pretrained_lstm.ckpt-14000\n",
      "saved to models/pretrained_lstm.ckpt-16000\n",
      "saved to models/pretrained_lstm.ckpt-18000\n",
      "saved to models/pretrained_lstm.ckpt-20000\n",
      "saved to models/pretrained_lstm.ckpt-22000\n",
      "saved to models/pretrained_lstm.ckpt-24000\n",
      "saved to models/pretrained_lstm.ckpt-26000\n",
      "saved to models/pretrained_lstm.ckpt-28000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    # TODO 3.5\n",
    "    # Get next training batch\n",
    "    nextBatch, nextBatchLabels = getTrainBatch()\n",
    "    # Feed to optimizer\n",
    "    ...\n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {inputs: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    # Save model every 2000 training iterations\n",
    "    if (i % 2000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, os.path.join(currentDir,\"models/pretrained_lstm.ckpt\"), global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load mô hình đã train và đánh giá mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thời gian huấn luyện mạng khá lâu, nên trong quá trình mạng đang được huấn luyện, ta sẽ lưu lại một số checkpoint. Để có thể test thử trên một checkpoint mới nhất ta sử dụng hàm tf.train.latest_checkpoint và truyền vào tên thư mục muốn lấy model mới nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-28000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "model = saver.restore(sess, tf.train.latest_checkpoint(os.path.join(currentDir,'models')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó, với mỗi batch dữ liệu test, ta sẽ tiến hành test và tính độ chính xác"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.6: Test mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7fbbc641f670>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test on 10 batches\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch()\n",
    "    # TODO 3.6: Tính độ chính xác 'accuracy' trên các test batch và gán vào 'test_acc'\n",
    "    sess.run(accuracy, feed_dict={inputs: nextBatch, labels: nextBatchLabels})\n",
    "#     test_acc = 0\n",
    "    print(\"Accuracy for this batch:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do các bộ test được lấy ngẫu nhiên nên độ chính xác trong quá trình này cũng dao động từ 70% đến 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.7: Viết hàm tổng hợp để dự đoán cảm xúc từ câu tiếng Việt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu cuối cùng này đòi hỏi đòi hỏi các bạn phải vận dụng tư duy tổng hợp để gom tất cả những bước đã thực hiện trước đó thành một quy trình hoàn chỉnh. Các bạn cần viết một hàm hoàn chỉnh với đầu vào là  một câu tiếng Việt cho trước, đầu ra là cho biết câu trên có cảm xúc tích cực hay tiêu cực."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"Placeholder:0\", shape=(64, 180), dtype=int32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(64, 180), dtype=int32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-c861a6958ab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# để dự đoán xem câu này có cảm xúc tích cực hay tiêu cực\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Câu này làm khá dài và có tính chất tổng hợp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m             raise TypeError('The value of a feed cannot be a tf.Tensor object. '\n\u001b[0m\u001b[1;32m   1138\u001b[0m                             \u001b[0;34m'Acceptable feed values include Python scalars, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                             \u001b[0;34m'strings, lists, numpy ndarrays, or TensorHandles. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"Placeholder:0\", shape=(64, 180), dtype=int32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(64, 180), dtype=int32)."
     ]
    }
   ],
   "source": [
    "input_sentence = ['Món này ăn ngon mê ly luôn. Vị ngọt và thơm quá trời quá đất.'] \n",
    "# TODO 3.7 Các bạn vận dụng toàn bộ quy trình đã thực hiện trước đó\n",
    "# để dự đoán xem câu này có cảm xúc tích cực hay tiêu cực\n",
    "# Câu này làm khá dài và có tính chất tổng hợp\n",
    "sess.run(prediction, {inputs: inputs})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy qua bài tập này, các bạn được ôn lại mô hình Word2Vec và sử dụng mô hình này để biểu diễn cho một văn bản. Sử dụng cách biểu diễn này để đưa vào mô hình RNN với nhiều đơn vị LSTM. Các bạn có thể thử nghiệm trên các cấu hình khác nhau bằng cách thay đổi các hyperparameter."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
