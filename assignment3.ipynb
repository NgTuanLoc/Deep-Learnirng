{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T·∫°o m√°y ·∫£o b·∫±ng Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kill kernel c≈© ƒëi ƒë·ªìng th·ªùi t·∫°o k·∫øt n·ªëi m·ªõi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt v√† ch·ª©ng th·ª±c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "# !apt-get update -qq 2>&1 > /dev/null\n",
    "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# creds = GoogleCredentials.get_application_default()\n",
    "# import getpass\n",
    "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "# vcode = getpass.getpass()\n",
    "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán li√™n quan ƒë·∫øn vi·ªác k·∫øt n·ªëi Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p drive\n",
    "# !google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ph√¢n t√≠ch c·∫£m x√∫c v·ªõi LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment n√†y, ch√∫ng ta s·∫Ω d√πng m·∫°ng LSTM ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n ph√¢n t√≠ch c·∫£m x√∫c (Sentiment Analysis) tr√™n t·∫≠p d·ªØ li·ªáu vƒÉn b·∫£n. N·∫øu nh√¨n theo ki·ªÉu black box, ƒë·∫ßu v√†o c·ªßa b√†i to√°n l√† m·ªôt c√¢u ho·∫∑c ƒëo·∫°n vƒÉn b·∫£n v√† ƒë·∫ßu ra l√† tr·∫°ng th√°i t√≠ch c·ª±c, ti√™u c·ª±c hay trung ho√† (positive - negative - neutral). Trong ph·∫°m vi c·ªßa assignment n√†y, ch√∫ng ta ch·ªâ quan t√¢m ƒë·∫øn hai tr·∫°ng th√°i c·∫£m x√∫c l√† positive v√† negative.\n",
    "\n",
    "![caption](Images/input_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G√≥c nh√¨n Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N·∫øu nh∆∞ ch√∫ng ta gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng ƒë·∫ßu v√†o l√† chu·ªói k√Ω t·ª± th√¨ r·∫•t kh√≥ ƒë·ªÉ th·ª±c hi·ªán c√°c thao t√°c bi·∫øn ƒë·ªïi nh∆∞ t√≠ch v√¥ h∆∞·ªõng (dot product) ho·∫∑c c√°c thu·∫≠t to√°n tr√™n m·∫°ng neural network nh∆∞ backpropagation. Thay v√¨ d·ªØ li·ªáu ƒë·∫ßu v√†o l√† m·ªôt chu·ªói, ch√∫ng ta c·∫ßn chuy·ªÉn ƒë·ªïi c√°c t·ª´ trong t·∫≠p t·ª´ ƒëi·ªÉn sang d·∫°ng vector s·ªë h·ªçc trong ƒë√≥ c√≥ th·ªÉ th·ª±c hi·ªán ƒë∆∞·ª£c c√°c ph√©p to√°n n√™u tr√™n.\n",
    "\n",
    "![caption](Images/word2vec.png)\n",
    "\n",
    "Trong h√¨nh minh ho·∫° ·ªü tr√™n, ta c√≥ th·ªÉ h√¨nh dung d·ªØ li·ªáu ƒë·∫ßu v√†o c·ªßa thu·∫≠t to√°n ph√¢n t√≠ch c·∫£m x√∫c l√† m·ªôt ma tr·∫≠n 16 x D chi·ªÅu. Trong ƒë√≥ 16 l√† s·ªë l∆∞·ª£ng t·ª´ trong c√¢u v√† D l√† s·ªë chi·ªÅu c·ªßa kh√¥ng gian vector ƒë·ªÉ bi·ªÉu di·ªÖn t·ª´. ƒê·ªÉ √°nh x·∫° t·ª´ m·ªôt t·ª´ sang m·ªôt vector, ch√∫ng ta s·ª≠ d·ª•ng ma tr·∫≠n word embedding nh∆∞ ƒë√£ th·ª±c hi·ªán trong b√†i Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T·∫≠p d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment n√†y, ch√∫ng t√¥i s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu review tr√™n trang Foody v·ªõi kho·∫£ng 30,000 m·∫´u ƒë∆∞·ª£c g√°n nh√£n. Trong ƒë√≥ c√≥ 15,000 m·∫´u positive v√† 15,000 m·∫´u negative. Ngu·ªìn: https://streetcodevn.com/blog/dataset. T·∫≠p d·ªØ li·ªáu n√†y ƒë√£ ƒë∆∞·ª£c ƒë√≠nh k√®m trong th∆∞ m·ª•c c·ªßa assignment 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√°c b∆∞·ªõc ƒë·ªÉ hu·∫•n luy·ªán tr√™n m·∫°ng RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C√≥ 5 b∆∞·ªõc ch√≠nh ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n ph√¢n t√≠ch c·∫£m x√∫c trong vƒÉn b·∫£n:\n",
    "\n",
    "    1) Hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh ph√°t sinh ra vector t·ª´ (nh∆∞ m√¥ h√¨nh Word2Vec) ho·∫∑c t·∫£i l√™n c√°c vector t·ª´ ti·ªÅn hu·∫•n luy·ªán.\n",
    "    2) T·∫°o ma tr·∫≠n ID cho t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán\n",
    "    3) T·∫°o m√¥ h√¨nh RNN v·ªõi c√°c ƒë∆°n v·ªã LSTM, s·ª≠ d·ª•ng tensorflow\n",
    "    4) Hu·∫•n luy·ªán m√¥ h√¨nh RNN v·ªõi d·ªØ li·ªáu ma tr·∫≠n ƒë√£ t·∫°o ·ªü b∆∞·ªõc 2\n",
    "    5) ƒê√°nh gi√° m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán v·ªõi t·∫≠p test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load t·∫≠p t·ª´ v·ª±ng v√† ma tr·∫≠n word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·∫ßu ti√™n, ƒë·ªÉ c√≥ th·ªÉ bi·∫øn ƒë·ªïi m·ªôt t·ª´ th√†nh m·ªôt vector, ch√∫ng ta s·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc ƒë√≥ (pretrained model). M√¥ h√¨nh ƒë√£ train tr∆∞·ªõc ƒë√≥ cho ti·∫øng Vi·ªát ƒë∆∞·ª£c l·∫•y ·ªü ƒë√¢y: https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.vi.300.vec.gz\n",
    "\n",
    "Tuy nhi√™n, s·ªë l∆∞·ª£ng t·ª´ v·ª±ng ti·∫øng Vi·ªát ƒë∆∞·ª£c hu·∫•n luy·ªán r·∫•t l·ªõn, kho·∫£ng 2M t·ª´. M·ªói t·ª´ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng m·ªôt vector 300 chi·ªÅu. V·ªõi k√≠ch th∆∞·ªõc g·ªëc c·ªßa ma tr·∫≠n word embedding nh∆∞ v·∫≠y s·∫Ω g√¢y kh√≥ khƒÉn cho vi·ªác load d·ªØ li·ªáu c≈©ng nh∆∞ ƒë∆∞a v√†o th∆∞ vi·ªán tensorflow ƒë·ªÉ hu·∫•n luy·ªán n√™n ch√∫ng t√¥i ƒë√£ t·ªëi gi·∫£n l·∫°i v·ªõi s·ªë l∆∞·ª£ng t·ª´ t·ªëi thi·ªÉu ƒë·ªÉ c√≥ th·ªÉ ch·∫°y ƒë∆∞·ª£c tr√™n t·∫≠p d·ªØ li·ªáu review v·ªÅ ƒë·ªì ƒÉn c·ªßa Foody.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# L∆ØU √ù: C·∫¶N PH·∫¢I CH·ªàNH L·∫†I ƒê∆Ø·ªúNG D·∫™N N√ÄY TH√ÄNH TH∆Ø M·ª§C CH·ª®A C√ÅC FILE ASSIGNMENT3\n",
    "# CH·ªÆ 'drive' c√≥ nghƒ©a l√† th∆∞ m·ª•c m·∫∑c ƒë·ªãnh c·ªßa Google drive\n",
    "currentDir = ''\n",
    "\n",
    "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "print('Simplified vocabulary loaded!')\n",
    "wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ ch·∫Øc ch·∫Øn m·ªçi d·ªØ li·ªáu ƒë∆∞·ª£c load l√™n m·ªôt c√°ch ch√≠nh x√°c, ch√∫ng ta c·∫ßn ki·ªÉm tra xem s·ªë l∆∞·ª£ng t·ª´ trong t·ª´ ƒëi·ªÉn r√∫t g·ªçn v√† s·ªë chi·ªÅu c·ªßa ma tr·∫≠n word embedding c√≥ kh·ªõp v·ªõi nhau hay kh√¥ng? Trong tr∆∞·ªùng h·ª£p n√†y s·ªë t·ª´ m√† ch√∫ng t√¥i gi·ªØ l·∫°i l√† 19,899 v√† s·ªë chi·ªÅu trong kh√¥ng gian bi·ªÉu di·ªÖn l√† 300 chi·ªÅu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary:  19899\n",
      "Size of the word embedding matrix:  (19899, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Size of the vocabulary: ', len(wordsList))\n",
    "print('Size of the word embedding matrix: ', wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec tr√™n m·ªôt t·ª´ ƒë∆°n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ c√≥ th·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c vector bi·ªÉu di·ªÖn c·ªßa m·ªôt t·ª´ ti·∫øng Vi·ªát. ƒê·∫ßu ti√™n ch√∫ng ta s·∫Ω x√°c ƒë·ªãnh xem v·ªã tr√≠ c·ªßa t·ª´ ƒë√≥ trong wordsList. Sau ƒë√≥ l·∫•y vector ·ªü d√≤ng t∆∞∆°ng ·ª©ng tr√™n tr√™n ma tr·∫≠n wordVectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of `ngon` in wordsList:  14598\n"
     ]
    }
   ],
   "source": [
    "ngon_idx = wordsList.index('ngon')\n",
    "print('Index of `ngon` in wordsList: ', ngon_idx)\n",
    "# ngon_vec = wordVectors[ngon_idx]\n",
    "# print('Vector representation of `ngon` is: ', ngon_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.1: Word2Vec ƒë·ªÉ bi·ªÉu di·ªÖn m·ªôt ƒëo·∫°n vƒÉn b·∫£n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N√¢ng c·∫•p h∆°n so v·ªõi phi√™n b·∫£n Word2Vec cho t·ª´ ƒë∆°n, ph·∫ßn n√†y ch√∫ng ta s·∫Ω bi·ªÉu di·ªÖn m·ªôt c√¢u d∆∞·ªõi d·∫°ng m·ªôt ma tr·∫≠n g·ªìm c√°c vector bi·ªÉu di·ªÖn c·ªßa t·ª´ng t·ª´ ch·ªìng l√™n nhau.\n",
    "\n",
    "V√≠ d·ª• nh∆∞ ch√∫ng ta mu·ªën bi·ªÉu di·ªÖn c√¢u \"M√≥n n√†y ƒÉn ho√†i kh√¥ng bi·∫øt ch√°n\". ƒê·∫ßu ti√™n, v·ªõi m·ªói t·ª´ trong c√¢u ta s·∫Ω t√¨m ch·ªâ s·ªë t∆∞∆°ng ·ª©ng trong t·ª´ ƒëi·ªÉn v√† l∆∞u v√†o vector ƒë·∫∑t t√™n l√† 'sentenceIndexes'. Sau ƒë√≥, ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng h√†m tra c·ª©u ma tr·∫≠n word embedding c·ªßa th∆∞ vi·ªán Tensorflow tf.nn.embedding_lookup ƒë·ªÉ tra c√°c vector t·∫°i c√°c ch·ªâ s·ªë trong 'sentenceIndexes'. Nh∆∞ v·∫≠y n·∫øu ch√∫ng ta s·ª≠ d·ª•ng t·ªëi ƒëa 10 t·ª´ ƒë·ªÉ l∆∞u tr·ªØ cho m·ªôt c√¢u th√¨ ma tr·∫≠n bi·ªÉu di·ªÖn cho c√¢u s·∫Ω l√† m·ªôt ma tr·∫≠n k√≠ch th∆∞·ªõc 10 x 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Row index for each word:  [  119  8136  4884 18791 16614 15951  3371     0     0     0]\n",
      "Sentence representation of word vectors:\n",
      "(10, 300)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "maxSeqLength = 10   #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "sentenceIndexes = np.zeros((maxSeqLength), dtype='int32')\n",
    "\n",
    "# TODO 3.1: G√°n ch·ªâ s·ªë c·ªßa c√°c t·ª´ trong c√¢u v√† 'sentenceIndexes'\n",
    "my_sentence = 'M√≥n n√†y ƒÉn ho√†i kh√¥ng bi·∫øt ch√°n'\n",
    "def return_words_id(sentence):\n",
    "    return [wordsList.index(id) for id in sentence.lower().split(\" \")]\n",
    "\n",
    "for i, item in enumerate(return_words_id(my_sentence)):\n",
    "    sentenceIndexes[i] = item\n",
    "\n",
    "# C√°c ch·ªâ s·ªë 7, 8, 9 c·ªßa sentenceIndexes  v·∫´n ƒë∆∞·ª£c g√°n b·∫±ng 0 nh∆∞ c≈©\n",
    "print(sentenceIndexes.shape)\n",
    "print('Row index for each word: ', sentenceIndexes)\n",
    "\n",
    "# Ma tr·∫≠n bi·ªÉu di·ªÖn:\n",
    "print('Sentence representation of word vectors:')\n",
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,sentenceIndexes).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N·∫øu nh∆∞ th·ª±c hi·ªán ƒë√∫ng th√¨ vector 'sentenceIndexes' s·∫Ω c√≥ gi√° tr·ªã l√†: [119, 8136, 4884, 18791, 16614, 15951, 3371, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kh·∫£o s√°t t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán v√† t·∫°o ma tr·∫≠n ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment 3, ch√∫ng t√¥i s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu l·∫•y t·ª´ trang web Foody tr√™n mi·ªÅn d·ªØ li·ªáu li√™n quan ƒë·∫øn ·∫©m th·ª±c. T·∫≠p d·ªØ li·ªáu bao g√¥m 15.000 review t√≠ch c·ª±c ƒë·∫∑t trong th∆∞ m·ª•c 'positiveReviews' v√† 15.000 review ti√™u c·ª±c ƒë·∫∑t trong th∆∞ m·ª•c 'negativeReviews'. Do kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu l·ªõn, n·∫øu ch√∫ng ta ch·ªçn s·ªë l∆∞·ª£ng t·ª´ t·ªëi ƒëa (maxSeqLength) qu√° cao th√¨ s·∫Ω b·ªã l√£ng ph√≠ khi bi·ªÉu di·ªÖn ·ªü nh·ªØng c√¢u review qu√° ng·∫Øn. Ng∆∞·ª£c l·∫°i, n·∫øu s·ª≠ d·ª•ng s·ªë l∆∞·ª£ng t·ª´ t·ªëi ƒëa qu√° √≠t th√¨ s·∫Ω b·ªã b·ªè l·ª° nh·ªØng t·ª´ quan tr·ªçng gi√∫p cho vi·ªác ph√¢n t√≠ch c·∫£m x√∫c.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 30000\n",
      "The total number of words in the files is 1770824\n",
      "The average number of words in the files is 59.02746666666667\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positiveReviews/27869.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positiveFiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11010"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positiveFiles.index('positiveReviews/1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng th∆∞ vi·ªán Matplot ƒë·ªÉ minh ho·∫° ph√¢n b·ªë v·ªÅ chi·ªÅu d√†i c·ªßa c√°c c√¢u review trong t·∫≠p d·ªØ li·ªáu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D·ª±a tr√™n bi·ªÉu ƒë·ªì histogram ·ªü tr√™n ch√∫ng ta c√≥ th·ªÉ th·∫•y l√† 180 l√† k·∫øt qu·∫£ t∆∞∆°ng ƒë·ªëi h·ª£p l√Ω. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "# import re\n",
    "# strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
    "\n",
    "# def cleanSentences(string):\n",
    "#     string = string.lower().replace(\"<br />\", \" \")\n",
    "#     return re.sub(strip_special_chars, \"\", string.lower())\n",
    "import re\n",
    "\n",
    "def remove_punc_generator(string):\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for ele in string:  \n",
    "        if ele in punc:  \n",
    "            string = string.replace(ele, \" \") \n",
    "    yield string\n",
    " \n",
    " \n",
    "# sample = test\n",
    " \n",
    "# sample = remove_punc_generator(sample)\n",
    "# re.findall(r'\\S+', next(sample))\n",
    "def cleanSentences(string):\n",
    "    string = string.lower()\n",
    "    string = remove_punc_generator(string)\n",
    "#     string = \n",
    "    return re.findall(r'\\S+', next(string))\n",
    "# cleanSentences(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ c√≥ c·∫£m nh·∫≠n r√µ h∆°n v·ªÅ d·ªØ li·ªáu, ch√∫ng ta c√≥ th·ªÉ hi·ªÉn th·ªã m·ªôt s·ªë review b·∫•t k·ª≥ nh∆∞ sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A positive sentence: \n",
      "m√¨nh bi·∫øt qu√°n qua foody v√† m·ªõi ƒëi ƒÉn h√¥m qua lu√¥n m√¨nh g·ªçi kem x√¥i xo√†i 27k x√∫c x√≠ch v√† ph√¥ mai que kh√¥ng nh·ªõ gi√°\n",
      "kem x√¥i h∆°i b√© so v·ªõi m√¨nh t∆∞·ªüng t∆∞·ª£ng c·ª•c x√¥i ƒë·ª´og k√≠nh kho·∫£ng 5cm c√≤n kem th√¨ b√© h∆°n x√≠u nh∆∞ng v·ªã ƒÉn r·∫•t ngon ban ƒë·∫ßu ƒÉn th·∫•y h∆°i ng·ªçt nh∆∞ng n·∫øu ƒÉn chung v·ªõi ƒë·∫≠u ph·ªông n·ªØa th√¨ tuy·ªát v·ªùi lu√¥n üëç üëç\n",
      "x√∫c x√≠ch v·ªõi ph√¥ mai que beo b√©o nchung l√† ngon üíï üíï\n",
      "ph·ª•c v·ª• th√¨ kh√¥ng c√≥ g√¨ ph·∫£i ch√™ c·∫£ t·ª´ ch·ªã ·ªü qu·∫ßy ƒë·∫øn m·∫•y anh ph·ª•c v·ª• tr√™n l·∫ßu c≈©ng r·∫•t vui v·∫ª ni·ªÅm n·ªü kh√¥ng gian qu√°n s·∫°ch s·∫Ω tho√°ng m√°t v√† decor c≈©ng ƒë·∫πp\n",
      "ch·ªâ c√≥ 1 t√≠ m√¨nh mu·ªën g√≥p √Ω l√† b√†n c√≥ ki·∫øn bu kh√° l√† nhi·ªÅu v·∫≠y th√¥ii n·∫øu c√≥ d·ªãp th√¨ ch·∫Øc ch·∫Øn m√¨nh s·∫Ω t·ªõi ti·∫øp üëç üëç\n",
      "A negative sentence: \n",
      "Qu√°n ƒë·ªì_ƒÉn kh√° ngon . . nh∆∞ng ph·ª•c_v·ª• kh√¥ng t·ªët . . ch·ªâ lo s·ª≠a_so·∫°n . . kh√¥ng ƒë·ªÉ_√Ω t·ªõi kh√°ch . . ƒë·ªì_ƒÉn l√†m r·∫•t l√¢u trong khi ch·ªâ c√≥ 2 b√†n . . g·∫ßn 1 ti·∫øng m√† ch∆∞a ra . . trong khi ch·ªâ c√≥ rau x√†o v√† g·ªèi . khi m√¨nh ƒë·ª£i qu√° l√¢u n√™n t√≠nh ti·ªÅn th√¨ qu·∫£n_l√Ω c√≥ th√°i_ƒë·ªô kh√≥_ch·ªãu . . kh√¥ng th√≠ch th√°i_ƒë·ªô ph·ª•c_v·ª• 1 ch√∫t n√†o . . ƒë√¢y l√† l·∫ßn cu·ªëi_c√πng gh√© qu√°n . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('A positive sentence: ')\n",
    "fname = positiveFiles[1] # Randomly select a positive file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    id \n",
    "    test = []\n",
    "    for lines in f:\n",
    "        cleanedLine = cleanSentences(lines)\n",
    "        print(' '.join(cleanedLine))\n",
    "#         cleanedLine = cleanSentences(line)\n",
    "\n",
    "print('A negative sentence: ')\n",
    "fname = negativeFiles[0] # Randomly select a negative file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chu·∫©n ho√° vƒÉn b·∫£n v√† t√°ch t·ª´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ ti·∫øt ki·ªám c√¥ng s·ª©c v√† c≈©ng n·∫±m ngo√†i ph·∫°m vi c·ªßa kho√° h·ªçc, ch√∫ng t√¥i ƒë√£ chu·∫©n b·ªã s·∫µn t·∫≠p d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t√°ch t·ª´. Gi·ªØa hai t·ª´ c√≥ th·ªÉ gh√©p l·∫°i ƒë·ªÉ t·∫°o th√†nh m·ªôt kh√°i ni·ªám m·ªõi ch√∫ng t√¥i s·ª≠ d·ª•ng k√Ω t·ª± '_' ƒë·ªÉ n·ªëi c√°c t·ª´ ƒë√≥. V√≠ d·ª•: 'sinh_vi√™n', 'sinh_h·ªçc'.\n",
    "\n",
    "Ch√∫ng t√¥i chu·∫©n b·ªã s·∫µn c√°c h√†m chu·∫©n ho√° vƒÉn b·∫£n nh·∫±m lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát. Tham kh·∫£o ·ªü h√†m 'cleanSentences'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B√¢y gi·ªù ch√∫ng ta s·∫Ω bi·ªÉu di·ªÖn 30.000 review d∆∞·ªõi d·∫°ng c√°c ch·ªâ s·ªë c·ªßa c√°c t·ª´. T·∫≠p d·ªØ li·ªáu positive v√† negative s·∫Ω ƒë∆∞·ª£c t√≠nh h·ª£p l·∫°i th√†nh m·ªôt ma tr·∫≠n 30000x180. Trong ƒë√≥ 30000 l√† s·ªë l∆∞·ª£ng review v√† 180 l√† s·ªë l∆∞·ª£ng t·ª´ t·ªëi ƒëa cho m·ªôt c√¢u. Do b∆∞·ªõc chu·∫©n b·ªã n√†y t·ªën kh√° nhi·ªÅu t√†i nguy√™n t√≠nh to√°n n√™n sau khi t√≠nh to√°n xong, ch√∫ng ta s·∫Ω l∆∞u l·∫°i ƒë·ªÉ s·ª≠ d·ª•ng cho nh·ªØng l·∫ßn ch·∫°y th√≠ nghi·ªám sau. Ma tr·∫≠n l∆∞u tr·ªØ c√°c ch·ªâ s·ªë n√†y l√†: 'ids'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.2: x√°c ƒë·ªãnh ch·ªâ s·ªë c·ªßa t·ª´ng t·ª´ trong review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ph·∫ßn n√†y ch√∫ng ta s·∫Ω ti·∫øn h√†nh tra c·ª©u t·ª´ng t·ª´ trong review, sau ƒë√≥ g√°n v√†o ma tr·∫≠n 'ids'. Trong ƒë√≥ ch·ªâ s·ªë d√≤ng c·ªßa ma tr·∫≠n t∆∞∆°ng ·ª©ng v·ªõi file review, ch·ªâ s·ªë c·ªôt c·ªßa ma tr·∫≠n t∆∞∆°ng ·ª©ng v·ªõi m·ªôt t·ª´ c·ªßa review. Tr∆∞·ªùng h·ª£p t·ª´ n√†o kh√¥ng c√≥ trong t·∫≠p t·ª´ ƒëi·ªÉn th√¨ ta s·∫Ω g√°n b·∫±ng ch·ªâ s·ªë c·ªßa t·ª´ 'UNK' (unknow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files are indexed!\n",
      "Negative files are indexed!\n"
     ]
    }
   ],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "nFiles = 0\n",
    "# Index of Unknow word\n",
    "unk_idx = wordsList.index('UNK')\n",
    "\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
    "        nIndexes = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "#         split = cleanedLine.split()\n",
    "        split = cleanedLine\n",
    "        for word in split:\n",
    "            # TODO 3.2: N·∫øu 'word' thu·ªôc t·∫≠p 'wordsList' th√¨ g√°n ch·ªâ s·ªë c·ªßa 'word' v√†o ma tr·∫≠n ids\n",
    "            if (word in wordsList):\n",
    "                ids[nFiles, nIndexes] = wordsList.index(word)\n",
    "            # Ng∆∞·ª£c l·∫°i: g√°n 'unk_idx' v√†o ma tr·∫≠n ids\n",
    "            else:\n",
    "                ids[nFiles, nIndexes] = unk_idx\n",
    "                \n",
    "            nIndexes = nIndexes + 1\n",
    "            if nIndexes >= maxSeqLength:\n",
    "                break\n",
    "        nFiles = nFiles + 1 \n",
    "print('Positive files are indexed!')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding=\"utf-8\") as f:\n",
    "        nIndexes = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "#         split = cleanedLine.split()\n",
    "        split = cleanedLine\n",
    "        for word in split:\n",
    "            if (word in wordsList):\n",
    "                ids[nFiles, nIndexes] = wordsList.index(word)\n",
    "            # Ng∆∞·ª£c l·∫°i: g√°n 'unk_idx' v√†o ma tr·∫≠n ids\n",
    "            else:\n",
    "                ids[nFiles, nIndexes] = unk_idx\n",
    "                        \n",
    "            nIndexes = nIndexes + 1\n",
    "            if nIndexes >= maxSeqLength:\n",
    "                break\n",
    "        nFiles = nFiles + 1 \n",
    "print('Negative files are indexed!')\n",
    "\n",
    "# Save ids Matrix for future uses.\n",
    "np.save(os.path.join(currentDir,'idsMatrix.npy'), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indexes of the first review:  [ 8589 18694  1906  4222  1232  5284 10661 11694 11994 18784 18569 15927\n",
      " 16416 13174  9821 14794  8884  6443  5767  8589 18423 12526 15570  5596\n",
      "   799 11060  4222 16893 13078  8136  3364  4222  1232  4756 15200  4884\n",
      "  8885  3553  9782  1232   616  7357 10606   579 15522  2219 15092 14855\n",
      " 15253  4884  3364  5519  4558  9649  4570 14653 15522 10959 12973 14855\n",
      "  4884 16614  2212  4884  7155 11577  4222  5767 15076 12225 10774  1218\n",
      "  2876 19584  4558 18839   255 13452  5013  4950  1825 10642 17292 11895\n",
      "   803 11060 16760  1906 15253 14598 15253  1047  5668  4884 10642 12225\n",
      "  7090 17292 18109 13078 16334  1238  3364  5519  3167 11318  3553 14967\n",
      "  4964 12067  2902  6544  6548  2997 14855  7446  8038 11440  1345  4950\n",
      "  1825  5767   803 11060 18791  5013     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# L∆ØU √ù: B∆∞·ªõc th·ª±c hi·ªán tr√™n t∆∞∆°ng ƒë·ªëi m·∫•t th·ªùi gian.\n",
    "# Tr∆∞·ªùng h·ª£p ƒë√£ t√≠nh to√°n v√† l∆∞u ma tr·∫≠n 'ids' r·ªìi th√¨ ta c√≥ th·ªÉ load l√™n ƒë·ªÉ s·ª≠ d·ª•ng lu√¥n\n",
    "ids = np.load(os.path.join(currentDir,'idsMatrix.npy'))\n",
    "print('Word indexes of the first review: ', ids[11010])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N·∫øu nh∆∞ qu√° tr√¨nh chuy·ªÉn t·ª´ c√¢u d·∫°ng vƒÉn b·∫£ng sang vector c√°c ch·ªâ s·ªë trong t·ª´ ƒëi·ªÉn ·ªü tr√™n ƒë√∫ng th√¨ ids[0] s·∫Ω nh·∫≠n gi√° tr·ªã: [19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821 ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  [19898, 1906, 4454, 5284, 10661, 11694, 11994, 18784, 18569, 18619, 13174, 9821]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M√≥n n√†y d·ª± l√† s·∫Ω th√†nh m√≥n hot m√πa l·ªÖ_h·ªôi n√†y ƒë√≥ nha . Mi·∫øng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK b√°n c∆°m_chi√™n n·∫±m ngay ƒë·∫ßu ƒë∆∞·ªùng v√†o khu d√¢n_c∆∞ metro ch·∫°y'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([wordsList[i] for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xe ƒë·∫©y b√°n c∆°m chi√™n n·∫±m ngay ƒë·∫ßu ƒë∆∞·ªùng v√†o khu d√¢n c∆∞ metro ch·∫°y t·ª´ ngo√†i v√¥ l√† xe th·ª© hai nh√© m√¨nh hay mua c∆°m c·ªßa ch·ªã n√†y l·∫Øm c∆°m chi√™n m·ªÅm n√≥ng ƒÉn chung v·ªõi tr·ª©ng chi√™n l·∫°p x∆∞·ªüng th·ªãt heo v√† ch√† b√¥ng n√™n v·ª´a ƒÉn l·∫Øm m√† c√≥ th√™m d∆∞a leo v√† c√† chua n√™n ƒÉn kh√¥ng ng√°n ƒÉn xong h·ª£p c∆°m l√† bao no ƒë·∫øn tr∆∞a gi√° ch·ªß c√≥ 10 1 h·ªôp √† bu·ªïi s√°ng r·∫•t ƒë√¥ng ng∆∞·ªùi gh√© mua v√¨ b√°n v·ª´a ngon v·ª´a r·∫ª l·∫°i ƒÉn r·∫•t no tuy ƒë√¥ng nh∆∞ng ch·ªã l√†m nhanh l·∫Øm m√† n√≥i chuy·ªán v·ªõi kh√°ch c≈©ng vui v·∫ª l·ªãch s·ª± n·ªØa n√™n l·∫ßn n√†o ƒëi ngang bu·ªïi s√°ng l√† gh√© mua ho√†i √† ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([wordsList[i] for i in ids[11010]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m√¨nh bi·∫øt qu√°n qua foody v√† m·ªõi ƒëi ƒÉn h√¥m qua lu√¥n m√¨nh g·ªçi kem x√¥i xo√†i 27k x√∫c x√≠ch v√† ph√¥ mai que kh√¥ng nh·ªõ gi√° ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm ƒÉm'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([wordsList[i] for i in ids[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X√¢y d·ª±ng h√†m l·∫•y d·ªØ li·ªáu train v√† test theo t·ª´ng batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D∆∞·ªõi ƒë√¢y ch√∫ng t√¥i x√¢y d·ª±ng c√°c h√†m ƒë·ªÉ l·∫•y d·ªØ li·ªáu train v√† test theo t·ª´ng batch. B·∫°n h√£y gi·∫£i th√≠ch t·∫°i sao l·∫°i c√≥ c√°c con s·ªë 13999, 14999, 15999, 29999 nh√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- T·∫≠p c√≥ nh√£n t√≠ch c·ª±c c√≥ id t·ª´ 0 t·ªõi 13 999\n",
    "- T·∫≠p c√≥ nh√£n ti√™u c·ª±c c√≥ id t·ª´ 15 999 t·ªõi 29 999\n",
    "- L·∫•y batch size l√† 15 999 - 13 999 = 2000 m·∫´u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            # Pick positive samples randomly\n",
    "            num = randint(1,13999)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            # Pick negative samples randomly\n",
    "            num = randint(15999,29999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(13999,15999)\n",
    "        if (num <= 14999):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. X√¢y d·ª±ng RNN Model v·ªõi Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·∫ßu ti√™n ch√∫ng t√¥i s·∫Ω kh·ªüi t·∫°o c√°c tham s·ªë cho m√¥ h√¨nh m·∫°ng RNN v·ªõi c√°c cell l√† c√°c LSTM. Ki·∫øn tr√∫c m·∫°ng ·ªü ƒë√¢y bao g·ªìm 128 ƒë∆°n v·ªã cho m·ªói l·ªõp, s·ªë l∆∞·ª£ng layer l√† 2, s·ªë l∆∞·ª£ng ph√¢n l·ªõp l√† 2 v√† s·ªë v√≤ng l·∫∑p khi hu·∫•n luy·ªán l√† 30000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paramters\n",
    "numDimensions = 300\n",
    "batchSize = 64\n",
    "lstmUnits = 128\n",
    "nLayers = 2\n",
    "numClasses = 2\n",
    "iterations = 30000\n",
    "maxSeqLength = 180 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu input v√† ouput, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng hai ki·ªÉu d·ªØ li·ªáu placeholder. M·ªôt trong nh·ªØng ƒëi·ªÅu quan tr·ªçng nh·∫•t khi kh·ªüi t·∫°o c√°c bi·∫øn input v√† output n√†y l√† x√°c ƒë·ªãnh k√≠ch th∆∞·ªõc c·ªßa c√°c tensor. M·ªói output c·ªßa m·∫°ng (hay c√≤n g·ªçi l√† label) s·∫Ω l√† m·ªôt vector one hot v·ªõi hai gi√° tr·ªã t∆∞∆°ng ·ª©ng v·ªõi hai lo·∫°i c·∫£m x√∫c: [1, 0] cho positive v√† [0, 1] cho negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/data_batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.3: X√°c ƒë·ªãnh input v√† output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kh·ªüi t·∫°o hai bi·∫øn 'inputs' v√† 'labels' b·∫±ng ki·ªÉu placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# TODO 3.3: Kh·ªüi t·∫°o hai bi·∫øn 'inputs' v√† 'labels'\n",
    "inputs = tf.placeholder(tf.int32, shape=(batchSize, maxSeqLength))\n",
    "labels = tf.placeholder(tf.float32, shape=(batchSize, numClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(64, 180) dtype=int32>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(64, 2) dtype=float32>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau ƒë√≥ t·∫°o d·ªØ li·ªáu word vector t·ª´ kh·ªëi d·ªØ li·ªáu ƒë·∫ßu v√†o v·ªõi ma tr·∫≠n word embedding. N·∫øu nh∆∞ qu√° tr√¨nh kh·ªüi t·∫°o ƒë√∫ng th√¨ s·∫Ω t·∫°o ra c√°c ki·ªÉu d·ªØ li·ªáu sau:\n",
    "labels --> Tensor(\"Placeholder:0\", shape=(64, 2), dtype=float32)\n",
    "inputs --> Tensor(\"Placeholder_1:0\", shape=(64, 10), dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19899, 300)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(64, 180) dtype=int32>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.nn.embedding_lookup(wordVectors, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(64, 180, 300) dtype=float32>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nh∆∞ v·∫≠y sau b∆∞·ªõc n√†y ch√∫ng ta ƒë√£ c√≥ d·ªØ li·ªáu ƒë·ªÉ ƒë∆∞a v√†o m·∫°ng m·∫°ng c√°c LSTM. ƒê·ªÉ kh·ªüi t·∫°o m·ªôt LSTM ch√∫ng ta s·ª≠ d·ª•ng h√†m tf.nn.rnn_cell.BasicLSTMCell. H√†m n√†y c·∫ßn tham s·ªë ƒë·∫ßu v√†o l√† s·ªë l∆∞·ª£ng ƒë∆°n v·ªã mu·ªën kh·ªüi t·∫°o. ƒê√¢y ch√≠nh l√† m·ªôt hyperparamter ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o tr∆∞·ªõc ƒë√≥.\n",
    "ƒê·ªÉ ch·ªëng l·∫°i vi·ªác overfitting, ch√∫ng ta s·ª≠ d·ª•ng l·ªõp dropout. \n",
    "\n",
    "ƒê·ªÉ tƒÉng t√≠nh ph·ª©c t·∫°p cho ki·∫øn tr√∫c m·∫°ng ch√∫ng ta ch·ªìng c√°c l·ªõp LSTM l√™n nhau (Stack LSTM Layers). Trong tr∆∞·ªùng h·ª£p n√†y ch√∫ng ta s·ª≠ d·ª•ng 2 l·ªõp LSTM. Vi·ªác ch·ªìng th√™m c√°c l·ªõp LSTM s·∫Ω gi√∫p cho m√¥ h√¨nh c√≥ kh·∫£ nƒÉng nh·ªõ nhi·ªÅu th√¥ng tin h∆°n nh∆∞ng ƒë·ªìng th·ªùi c≈©ng l√†m tƒÉng s·ªë l∆∞·ª£ng tham s·ªë khi hu·∫•n luy·ªán. ƒêi·ªÅu n√†y c≈©ng c√≥ nghƒ©a l√† s·∫Ω l√†m tƒÉng th·ªùi gian hu·∫•n luy·ªán c≈©ng nh∆∞ l√† c·∫ßn th√™m nhi·ªÅu d·ªØ li·ªáu h∆°n.\n",
    "\n",
    "Cu·ªëi c√πng l√† ƒë∆∞a to√†n b·ªô d·ªØ li·ªáu ƒë·∫ßu v√†o v√†o m·∫°ng c√°c LSTM s·ª≠ d·ª•ng h√†m tf.nn.dynamic_rnn. Chi ti·∫øt ki·∫øn tr√∫c m·∫°ng LSTM s·ª≠ d·ª•ng cho b√†i t·∫≠p n√†y ƒë∆∞·ª£c m√¥ t·∫£ trong h√¨nh sau:\n",
    "\n",
    "![caption](Images/architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:702: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "# Kh·ªüi t·∫°o m·ªôt LSTM layer v·ªõi 'lstmUnits' unit s·ª≠ d·ª•ng h√†m tf.contrib.rnn.BasicLSTMCell\n",
    "# Sau ƒë√≥ t·∫°o m·ªôt l·ªõp dropout ƒë·ªÉ ch·ªëng overfitting v·ªõi h·ªá s·ªë out_keep_prob b·∫±ng 0.75\n",
    "# S·ª≠ d·ª•ng h√†m tf.contrib.rnn.DropoutWrapper\n",
    "def make_cell():\n",
    "    lstmCell = tf.nn.rnn_cell.BasicLSTMCell(lstmUnits)\n",
    "    lstmCell = tf.nn.rnn_cell.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)    \n",
    "    return lstmCell\n",
    "\n",
    "\n",
    "def generate_a_lstm_layer(data, num_layers):\n",
    "    # stackedLstm = tf.nn.rnn_cell.MultiRNNCell([lstmCell, lstmCell])\n",
    "    stackedLstm = tf.nn.rnn_cell.MultiRNNCell([make_cell() for _ in range(num_layers)])\n",
    "\n",
    "    dyn_rnn_outputs, state = tf.nn.dynamic_rnn(stackedLstm, data, dtype=tf.float32)\n",
    "    return dyn_rnn_outputs\n",
    "\n",
    "# Sau khi ƒë√£ c√≥ h√†m t·∫°o m·ªôt LSTM Layer, ta s·ª≠ d·ª•ng h√†m n√†y ƒë·ªÉ ch·ªìng c√°c LSTM l√™n\n",
    "# Stack c√°c LSTM layer v·ªõi h√†m tf.nn.rnn_cell.MultiRNNCell\n",
    "...\n",
    "# Feed data variable v√†o m·∫°ng LSTM s·ª≠ d·ª•ng h√†m tf.nn.dynamic_rnn\n",
    "...\n",
    "# print(outputs)\n",
    "\n",
    "outputs = generate_a_lstm_layer(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(64, 180, 128) dtype=float32>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi ra kh·ªèi m·∫°ng LSTM, bi·∫øn outputs s·∫Ω l√† m·ªôt tensor c√≥ k√≠ch th∆∞·ªõc [batchSize x maxSeqLength x lstmUnits], c·ª• th·ªÉ l√† [64 x 180 x 128]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau ƒë√≥, ch√∫ng ta ch·ªâ l·∫•y d·ªØ li·ªáu ·ªü LSTM cell cu·ªëi c√πng v√† cho ƒëi qua l·ªõp k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß ƒë·ªÉ ph√¢n lo·∫°i th√†nh 2 tr·∫°ng th√°i. Ch·ªâ s·ªë c·ªßa LSTM cell cu·ªëi c√πng l√† 179 (do c√≥ 180 cell theo chi·ªÅu ngang)  n√™n ƒë·ªÉ c√≥ th·ªÉ l·∫•y ƒë∆∞·ª£c gi√° tr·ªã ta s·∫Ω chuy·ªÉn v·ªã v·ªÅ tensor c√≥ k√≠ch th∆∞·ªõc [maxSeqLength x batchSize x lstmUnits] hay [180 x 64 x 128]. S·ª≠ d·ª•ng h√†m tf.gather ƒë·ªÉ l·∫•y tensor th·ª© 179 c√≥ k√≠ch th∆∞·ªõc [64 x 128] bao g·ªìm 64 m·∫´u vector 128 chi·ªÅu. Vector 128 chi·ªÅu n√†y s·∫Ω ƒë∆∞·ª£c ƒë∆∞a v√†o l·ªõp fully connected ƒë·ªÉ chuy·ªÉn ƒë·ªïi v·ªÅ vector 2 chi·ªÅu t∆∞∆°ng ·ª©ng v·ªõi 2 tr·∫°ng th√°i.\n",
    "\n",
    "L·ªõp k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß bao g·ªìm c√°c b·ªô tham s·ªë 'weight' v√† 'bias' ƒë·ªÉ th·ª±c hi·ªán vi·ªác d·ª± ƒëo√°n k·∫øt qu·∫£. B∆∞·ªõc n√†y ch√≠nh l√† t·∫°o m·ªôt l·ªõp Fully Connected nh∆∞ trong s∆° ƒë·ªì ki·∫øn tr√∫c m·∫°ng LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "\n",
    "# L·∫•y gi√° tr·ªã output t·∫°i LSTM cell cu·ªëi c√πng\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "# last = tf.gather(outputs, int(value.get_shape()[0]) - 1)\n",
    "last = tf.gather(outputs, 180 - 1)\n",
    "# ƒê∆∞a qua m·∫°ng Fully Connected m√† kh√¥ng c√≥ activation function\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·ªÉ x√°c ƒë·ªãnh ƒë·ªô ch√≠nh x√°c c·ªßa h·ªá th·ªëng, ta ƒë·∫øm s·ªë l∆∞·ª£ng labels kh·ªõp v·ªõi gi√° tr·ªã d·ª± ƒëo√°n (prediction). Sau ƒë√≥ t√≠nh ƒë·ªô ch√≠nh x√°c b·∫±ng c√°ch t√≠nh gi√° tr·ªã trung b√¨nh c·ªßa c√°c k·∫øt qu·∫£ tr·∫£ v·ªÅ ƒë√∫ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctResult = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctResult, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau ƒë√≥ ch√∫ng ta s·∫Ω x√°c ƒë·ªãnh h√†m ƒë·ªô l·ªói s·ª≠ d·ª•ng softmax cross entropy ƒë∆∞·ª£c t√≠nh t·ª´ d·ªØ li·ªáu d·ª± ƒëo√°n v√† t·∫≠p labels. Cu·ªëi c√πng l√† ch·ªçn thu·∫≠t to√°n t·ªëi ∆∞u v·ªõi tham s·ªë learning rate m·∫∑c ƒë·ªãnh l√† 0.001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S·ª≠ d·ª•ng Tensorboard ƒë·ªÉ visualize k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong qu√° tr√¨nh hu·∫•n luy·ªán, ch∆∞∆°ng tr√¨nh s·∫Ω ghi log v·ªÅ ƒë·ªô l·ªói v√† ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p train v√†o th∆∞ m·ª•c 'tensorboard', l∆∞u l·∫°i model sau m·ªói 2000 v√≤ng l·∫∑p ·ªü th∆∞ m·ª•c 'models'. Vi·ªác hu·∫•n luy·ªán tr√™n 30,000 v√≤ng l·∫∑p m·∫•t kho·∫£ng v√†i ti·∫øng v·ªõi GPU K80 ƒë∆∞·ª£c cung c·∫•p b·ªüi Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V·ªõi m·ªói v√≤ng l·∫∑p, ta s·∫Ω l·∫•y ra m·ªôt batch d·ªØ li·ªáu train ƒë·ªÉ ƒë∆∞a v√†o m·∫°ng s·ª≠ d·ª•ng `feed_dict`. v·ªõi c√°c tham s·ªë input v√† label l√† c√°c placeholders. B∆∞·ªõc hu·∫•n luy·ªán n√†y ƒë∆∞·ª£c l·∫∑p l·∫°i cho ƒë·∫øn khi h·∫øt s·ªë l·∫ßn c·∫ßn hu·∫•n luy·ªán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/pretrained_lstm.ckpt-2000\n",
      "saved to models/pretrained_lstm.ckpt-4000\n",
      "saved to models/pretrained_lstm.ckpt-6000\n",
      "saved to models/pretrained_lstm.ckpt-8000\n",
      "saved to models/pretrained_lstm.ckpt-10000\n",
      "WARNING:tensorflow:From /Users/macintoshhd/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/saver.py:968: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "saved to models/pretrained_lstm.ckpt-12000\n",
      "saved to models/pretrained_lstm.ckpt-14000\n",
      "saved to models/pretrained_lstm.ckpt-16000\n",
      "saved to models/pretrained_lstm.ckpt-18000\n",
      "saved to models/pretrained_lstm.ckpt-20000\n",
      "saved to models/pretrained_lstm.ckpt-22000\n",
      "saved to models/pretrained_lstm.ckpt-24000\n",
      "saved to models/pretrained_lstm.ckpt-26000\n",
      "saved to models/pretrained_lstm.ckpt-28000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    # TODO 3.5\n",
    "    # Get next training batch\n",
    "    nextBatch, nextBatchLabels = getTrainBatch()\n",
    "    # Feed to optimizer\n",
    "    ...\n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {inputs: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    # Save model every 2000 training iterations\n",
    "    if (i % 2000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, os.path.join(currentDir,\"models/pretrained_lstm.ckpt\"), global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load m√¥ h√¨nh ƒë√£ train v√† ƒë√°nh gi√° m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th·ªùi gian hu·∫•n luy·ªán m·∫°ng kh√° l√¢u, n√™n trong qu√° tr√¨nh m·∫°ng ƒëang ƒë∆∞·ª£c hu·∫•n luy·ªán, ta s·∫Ω l∆∞u l·∫°i m·ªôt s·ªë checkpoint. ƒê·ªÉ c√≥ th·ªÉ test th·ª≠ tr√™n m·ªôt checkpoint m·ªõi nh·∫•t ta s·ª≠ d·ª•ng h√†m tf.train.latest_checkpoint v√† truy·ªÅn v√†o t√™n th∆∞ m·ª•c mu·ªën l·∫•y model m·ªõi nh·∫•t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-28000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "model = saver.restore(sess, tf.train.latest_checkpoint(os.path.join(currentDir,'models')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau ƒë√≥, v·ªõi m·ªói batch d·ªØ li·ªáu test, ta s·∫Ω ti·∫øn h√†nh test v√† t√≠nh ƒë·ªô ch√≠nh x√°c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.6: Test m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7fbbc641f670>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Accuracy for this batch: Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test on 10 batches\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch()\n",
    "    # TODO 3.6: T√≠nh ƒë·ªô ch√≠nh x√°c 'accuracy' tr√™n c√°c test batch v√† g√°n v√†o 'test_acc'\n",
    "    sess.run(accuracy, feed_dict={inputs: nextBatch, labels: nextBatchLabels})\n",
    "#     test_acc = 0\n",
    "    print(\"Accuracy for this batch:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do c√°c b·ªô test ƒë∆∞·ª£c l·∫•y ng·∫´u nhi√™n n√™n ƒë·ªô ch√≠nh x√°c trong qu√° tr√¨nh n√†y c≈©ng dao ƒë·ªông t·ª´ 70% ƒë·∫øn 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.7: Vi·∫øt h√†m t·ªïng h·ª£p ƒë·ªÉ d·ª± ƒëo√°n c·∫£m x√∫c t·ª´ c√¢u ti·∫øng Vi·ªát"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C√¢u cu·ªëi c√πng n√†y ƒë√≤i h·ªèi ƒë√≤i h·ªèi c√°c b·∫°n ph·∫£i v·∫≠n d·ª•ng t∆∞ duy t·ªïng h·ª£p ƒë·ªÉ gom t·∫•t c·∫£ nh·ªØng b∆∞·ªõc ƒë√£ th·ª±c hi·ªán tr∆∞·ªõc ƒë√≥ th√†nh m·ªôt quy tr√¨nh ho√†n ch·ªânh. C√°c b·∫°n c·∫ßn vi·∫øt m·ªôt h√†m ho√†n ch·ªânh v·ªõi ƒë·∫ßu v√†o l√†  m·ªôt c√¢u ti·∫øng Vi·ªát cho tr∆∞·ªõc, ƒë·∫ßu ra l√† cho bi·∫øt c√¢u tr√™n c√≥ c·∫£m x√∫c t√≠ch c·ª±c hay ti√™u c·ª±c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"Placeholder:0\", shape=(64, 180), dtype=int32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(64, 180), dtype=int32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-c861a6958ab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ƒë·ªÉ d·ª± ƒëo√°n xem c√¢u n√†y c√≥ c·∫£m x√∫c t√≠ch c·ª±c hay ti√™u c·ª±c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# C√¢u n√†y l√†m kh√° d√†i v√† c√≥ t√≠nh ch·∫•t t·ªïng h·ª£p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m             raise TypeError('The value of a feed cannot be a tf.Tensor object. '\n\u001b[0m\u001b[1;32m   1138\u001b[0m                             \u001b[0;34m'Acceptable feed values include Python scalars, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                             \u001b[0;34m'strings, lists, numpy ndarrays, or TensorHandles. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. For reference, the tensor object was Tensor(\"Placeholder:0\", shape=(64, 180), dtype=int32) which was passed to the feed with key Tensor(\"Placeholder:0\", shape=(64, 180), dtype=int32)."
     ]
    }
   ],
   "source": [
    "input_sentence = ['M√≥n n√†y ƒÉn ngon m√™ ly lu√¥n. V·ªã ng·ªçt v√† th∆°m qu√° tr·ªùi qu√° ƒë·∫•t.'] \n",
    "# TODO 3.7 C√°c b·∫°n v·∫≠n d·ª•ng to√†n b·ªô quy tr√¨nh ƒë√£ th·ª±c hi·ªán tr∆∞·ªõc ƒë√≥\n",
    "# ƒë·ªÉ d·ª± ƒëo√°n xem c√¢u n√†y c√≥ c·∫£m x√∫c t√≠ch c·ª±c hay ti√™u c·ª±c\n",
    "# C√¢u n√†y l√†m kh√° d√†i v√† c√≥ t√≠nh ch·∫•t t·ªïng h·ª£p\n",
    "sess.run(prediction, {inputs: inputs})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K·∫øt lu·∫≠n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nh∆∞ v·∫≠y qua b√†i t·∫≠p n√†y, c√°c b·∫°n ƒë∆∞·ª£c √¥n l·∫°i m√¥ h√¨nh Word2Vec v√† s·ª≠ d·ª•ng m√¥ h√¨nh n√†y ƒë·ªÉ bi·ªÉu di·ªÖn cho m·ªôt vƒÉn b·∫£n. S·ª≠ d·ª•ng c√°ch bi·ªÉu di·ªÖn n√†y ƒë·ªÉ ƒë∆∞a v√†o m√¥ h√¨nh RNN v·ªõi nhi·ªÅu ƒë∆°n v·ªã LSTM. C√°c b·∫°n c√≥ th·ªÉ th·ª≠ nghi·ªám tr√™n c√°c c·∫•u h√¨nh kh√°c nhau b·∫±ng c√°ch thay ƒë·ªïi c√°c hyperparameter."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
